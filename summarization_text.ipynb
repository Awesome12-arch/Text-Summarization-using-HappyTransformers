{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarization-public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeelBtOAMy9l"
      },
      "source": [
        "https://happytransformer.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1xOIz_BSFve"
      },
      "source": [
        "## Installation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCWRvlIIt732",
        "outputId": "79c80405-6799-44f1-f9f1-dc76f6bc8db2"
      },
      "source": [
        "pip install happytransformer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting happytransformer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e2/541eead178c066d843b0105aff73285fdc11438e8eaeb82efff6379fc690/happytransformer-2.2.4-py3-none-any.whl (43kB)\n",
            "\r\u001b[K     |███████▋                        | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 20kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 30kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 40kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (1.9.0+cu102)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 22.5MB/s \n",
            "\u001b[?25hCollecting transformers>=4.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/d5/c6c23ad75491467a9a84e526ef2364e523d45e2b0fae28a7cbe8689e7e84/transformers-4.8.1-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 44.2MB/s \n",
            "\u001b[?25hCollecting datasets>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/a2/d4e1024c891506e1cee8f9d719d20831bac31cb5b7416983c4d2f65a6287/datasets-1.8.0-py3-none-any.whl (237kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from happytransformer) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->happytransformer) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (4.5.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (3.13)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 48.1MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->happytransformer) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->happytransformer) (57.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.4.0->happytransformer) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.4.0->happytransformer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.4.0->happytransformer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.4.0->happytransformer) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=4.4.0->happytransformer) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.4.0->happytransformer) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.4.0->happytransformer) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.4.0->happytransformer) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.1)\n",
            "Installing collected packages: sentencepiece, huggingface-hub, sacremoses, tokenizers, transformers, xxhash, fsspec, datasets, happytransformer\n",
            "Successfully installed datasets-1.8.0 fsspec-2021.6.1 happytransformer-2.2.4 huggingface-hub-0.0.12 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.8.1 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2ezWDhPDpm"
      },
      "source": [
        "## Instantiation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIf5SNT7MwKH"
      },
      "source": [
        " from happytransformer import HappyTextToText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KlQTa4tM4X_",
        "outputId": "fb990e0c-39cd-4452-b70b-eeaf8c81dff3"
      },
      "source": [
        "happy_tt = HappyTextToText(\"DISTILBART\", \"sshleifer/distilbart-cnn-12-6\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/25/2021 22:20:46 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBSiaYkEPS9R"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "180-cPsON05i"
      },
      "source": [
        "# source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
        "text = \"A transformer is a deep learning model that adopts the mechanism of attention, differentially weighing the significance of each part of the input data. It is used primarily in the field of natural language processing (NLP)[1] and in computer vision (CV) Like recurrent neural networks (RNNs), transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, transformers do not necessarily process the data in order. Rather, the attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not need to process the beginning of the sentence before the end. Rather, it identifies the context that confers meaning to each word in the sentence. This feature allows for more parallelization than RNNs and therefore reduces training times.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0mX1g2_NI8r"
      },
      "source": [
        "result = happy_tt.generate_text(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDaeaAplOS1I",
        "outputId": "93e3e6f4-4c0b-410d-8be7-1a37d8760d4c"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextToTextResult(text=' A transformer is a deep learning model that adopts the mechanism of attention . It is used primarily in the field of natural language processing (NLP) and computer vision (CV)')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD81SGu-OX82",
        "outputId": "bde27da7-c918-4f5f-f81c-e8d0b3149eec"
      },
      "source": [
        "print(result.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " A transformer is a deep learning model that adopts the mechanism of attention . It is used primarily in the field of natural language processing (NLP) and computer vision (CV)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI5MQopKPU-K"
      },
      "source": [
        "## Adjust Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2QfpDDwOcjw"
      },
      "source": [
        "from happytransformer import TTSettings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGl6GdgEOcmz"
      },
      "source": [
        "top_k_sampling_settings = TTSettings(do_sample=True, top_k=50, temperature=0.7, max_length=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68MwNFohOcpY"
      },
      "source": [
        "result = happy_tt.generate_text(text, args=top_k_sampling_settings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlGhUHlMOnny",
        "outputId": "7c4d58f1-f099-4811-b1cb-d2c6dd8198a8"
      },
      "source": [
        "print(result.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " A transformer is a deep learning model that adopts the mechanism of attention . It uses the attention mechanism to differentially weight significance of each part of the input data . It is used primarily in the field of natural language processing .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0hHQeCKPPcg"
      },
      "source": [
        "## T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgDkUDtVOt1q",
        "outputId": "2fbc7b20-dc82-4d73-8d15-4f469e527d7f"
      },
      "source": [
        "happy_t5 = HappyTextToText(\"T5\", \"t5-base\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/25/2021 22:20:56 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaY_Is-cOcr-"
      },
      "source": [
        "input = \"summarize: \" + text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8UhmlGRPu_H"
      },
      "source": [
        "result = happy_t5.generate_text(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9DMF6FrPzMN",
        "outputId": "567002cc-09eb-4438-b2c2-fa92a3ff88a8"
      },
      "source": [
        "print(result.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transformers are deep learning models that adopt the mechanism of attention . they are designed to handle sequential input data for tasks such as translation . unlike recurrent neural networks, transformers do not necessarily process the data in order .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
